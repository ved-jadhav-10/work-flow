# Workflow AI â€” Persistent AI Context Layer

> **Workflow AI**: A production-ready AI assistant that actually remembers your project context â€” across documents, code, and conversations.

Workflow AI is an intelligent project management and AI assistance platform that maintains persistent context across all your work. Unlike traditional AI tools that forget previous interactions, Workflow AI builds a comprehensive understanding of your projects, learning materials, code, and tasks â€” providing context-aware assistance that evolves with your work.

## ğŸ¯ Vision

Most AI coding assistants suffer from **context amnesia** â€” every conversation starts from scratch. Workflow AI solves this by:

- **Persistent Memory**: Documents, code, and tasks feed a unified project context
- **Cross-Module Intelligence**: AI references your uploaded PDFs when explaining code
- **Smart Drift Detection**: Automatically flags responses that violate project constraints
- **Hybrid Inference**: Run on cloud (Gemini) or locally (Ollama) for privacy-first workflows
- **RAG-Powered Conversations**: Every query is augmented with relevant project knowledge

## ğŸš€ Features

### âœ… Currently Implemented

#### 1. **Authentication & User Management** (Phase 2)

- Email/password registration with bcrypt hashing
- GitHub OAuth integration
- JWT-based session management (custom React auth context)
- Protected routes via React Router guards
- User profile management with avatars

#### 2. **Project System** (Phase 3)

- Create and manage multiple projects
- Define project goals and constraints
- Track decisions and open questions
- JSONB-based flexible metadata storage
- Full CRUD operations with user isolation

#### 3. **Complete Database Schema**

All tables implemented with PostgreSQL + pgvector:

- **Users**: Authentication and profiles
- **Projects**: Core project metadata
- **Documents**: PDF storage and analysis
- **Code Insights**: Code explanations and suggestions
- **Tasks**: Priority-based task management
- **Embeddings**: Vector storage (768 dimensions via Gemini)
- **Chat Messages**: Conversation history with context tracking

#### 4. **LLM Abstraction Layer** (Phase 4)

- **Multi-Provider Support**: Gemini (primary), Groq (fallback), Ollama (local)
- **Automatic Fallback**: Switches providers on rate limits/failures
- **Unified Interface**: Single API for all LLM operations
- **Latency Tracking**: Performance monitoring per provider

#### 5. **PDF Processing Pipeline**

- PDF text extraction (PyMuPDF)
- Table extraction (pdfplumber)
- Intelligent chunking with overlap
- Embedding generation (text-embedding-004)
- Appwrite Storage integration

#### 6. **Learning Module** (Phase 5)

- **Document Upload**: Drag-and-drop PDF processing
- **Smart Summarization**: Short / Detailed / Exam-ready modes
- **Concept Extraction**: Key concepts with importance scores
- **Implementation Steps**: Actionable step-by-step plans
- **Vector Storage**: Automatic embedding generation for RAG

#### 7. **Developer Productivity Module** (Phase 6)

- **Code Explanation**: Structured analysis with components
- **Bug Detection**: Identifies issues with severity levels
- **README Generation**: Markdown documentation from code
- **Multi-Language Support**: Syntax highlighting and language detection
- **Code History**: Track previous insights

#### 8. **Workflow Automation** (Phase 7)

- **Task Extraction**: Parse meeting transcripts and emails
- **Priority Classification**: High / Medium / Low with AI reasoning
- **Task Management**: Update status, change priorities, delete tasks
- **Source Attribution**: Links tasks back to original text

### ğŸ”® Future Phases (Roadmap)

#### **Phase 8: Context Persistence Engine & RAG** (In Planning)

The core differentiator that ties everything together:

- **Unified Context API**: Aggregate data from all modules (docs, code, tasks)
- **Intelligent Retrieval**: Vector similarity search across all project content
- **Context-Aware Chat**: Every query augmented with relevant project knowledge
- **Source Attribution**: Show which documents/code influenced each response
- **Cross-Module Intelligence**: "How does my code implement concepts from my PDF?"
- **Context Health Score**: Visibility into knowledge graph completeness

**Key Features:**

```python
# Every AI response will be context-aware
response = query_with_context(
    project_id="...",
    query="What are my priorities?",
    # Automatically retrieves:
    # - Project goals and constraints
    # - Relevant document chunks
    # - Related code insights
    # - Open tasks by priority
)
```

#### **Phase 9: Drift Detection & Smart Features** (Planned)

Proactive intelligence that keeps your project on track:

- **Constraint Violation Detection**: AI responses that contradict project rules trigger warnings
- **Technology Stack Monitoring**: Flag mentions of disallowed frameworks/languages
- **Automatic Query Routing**: Detect intent and route to appropriate module
- **Decision Tracking**: AI-suggested decisions added to project context
- **False Positive Management**: Dismiss warnings that don't apply

**Example Drift Detection:**

```
Project Constraint: "React and TypeScript only"
User: "Should I use Vue for this component?"
AI Response: "Vue is a great choice for..."

âš ï¸ DRIFT WARNING: Response suggests Vue, but project is constrained to React
Violated Constraint: "React and TypeScript only"
Actions: [Dismiss] [Add to Decisions]
```

#### **Phase 10: Hybrid Inference & Production Deployment** (Planned)

Ship-ready platform with privacy-first local inference:

- **Cloud/Local Toggle**: Switch between Gemini and Ollama in settings
- **Privacy Mode**: Local inference with "data never leaves your machine" badge
- **Latency Comparison**: See cloud vs local performance in real-time
- **AMD Ryzen AI Ready**: Optimized for hardware acceleration
- **Production Landing Page**: Marketing site with demos
- **Full Deployment Pipeline**: Vercel (frontend) + Render (backend)
- **Error Handling Sweep**: Comprehensive validation and user feedback
- **Mobile Responsive**: Full tablet/phone support

**Settings Interface:**

```
Inference Mode: [Cloud â˜ï¸] [Local ğŸ”’]
Current Provider: Ollama (Local) - Privacy Mode Active
Avg Latency: 1.2s (Cloud: 0.4s)
Note: Local inference keeps all data on your machine
```

## ğŸ—ï¸ Architecture

### Technology Stack

#### Frontend (React + Vite)

- **Framework**: React 18+ with Vite build tool
- **Routing**: React Router DOM v6 (client-side SPA)
- **Auth**: Custom AuthContext (JWT stored in localStorage, auto-attached to API calls)
- **UI**: Tailwind CSS 4 via Vite plugin
- **HTTP Client**: Fetch API with centralized client (api.ts)
- **Syntax Highlighting**: react-syntax-highlighter
- **Icons**: lucide-react

#### Backend (FastAPI)

- **Framework**: FastAPI 0.115 (async/await)
- **Database**: Neon PostgreSQL with pgvector
- **ORM**: SQLAlchemy 2.0
- **Auth**: JWT via python-jose + bcrypt
- **File Storage**: Appwrite Cloud (private bucket, SDK-based)
- **AI Providers**:
  - Google Gemini (gemini-2.0-flash, text-embedding-004)
  - Groq (llama-3.1-70b-versatile)
  - Ollama (phi3:mini for local inference)
- **PDF Processing**: PyMuPDF + pdfplumber
- **Testing**: pytest + pytest-asyncio

#### Infrastructure

- **Database**: Neon (PostgreSQL 16 + pgvector extension)
- **File Storage**: Appwrite Cloud (private bucket)
- **Deployment** (Planned):
  - Frontend: Any static host (Vercel, Netlify, Cloudflare Pages)
  - Backend: Render (Dockerized)
  - Database: Neon (managed)
  - Storage: Appwrite (managed)

### Project Structure

```
work-flow/
â”œâ”€â”€ client/                    # React + Vite frontend (SPA)
â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â”œâ”€â”€ main.tsx          # App entry point
â”‚   â”‚   â”œâ”€â”€ App.tsx           # React Router setup
â”‚   â”‚   â”œâ”€â”€ pages/            # Page components
â”‚   â”‚   â”‚   â”œâ”€â”€ Landing.tsx           # Public landing page
â”‚   â”‚   â”‚   â”œâ”€â”€ Login.tsx             # Login page
â”‚   â”‚   â”‚   â”œâ”€â”€ Register.tsx          # Registration page
â”‚   â”‚   â”‚   â”œâ”€â”€ Dashboard.tsx         # Projects grid
â”‚   â”‚   â”‚   â”œâ”€â”€ NewProject.tsx        # Create project form
â”‚   â”‚   â”‚   â”œâ”€â”€ ProjectOverview.tsx   # Project detail with tabs
â”‚   â”‚   â”‚   â”œâ”€â”€ Learning.tsx          # Document module
â”‚   â”‚   â”‚   â”œâ”€â”€ Developer.tsx         # Code module
â”‚   â”‚   â”‚   â”œâ”€â”€ Workflow.tsx          # Task module
â”‚   â”‚   â”‚   â”œâ”€â”€ Chat.tsx              # AI conversation
â”‚   â”‚   â”‚   â””â”€â”€ Settings.tsx          # Inference settings
â”‚   â”‚   â”œâ”€â”€ components/
â”‚   â”‚   â”‚   â”œâ”€â”€ ui/               # Reusable UI primitives
â”‚   â”‚   â”‚   â””â”€â”€ layout/           # Sidebar, AuthGuard, layouts
â”‚   â”‚   â”œâ”€â”€ context/
â”‚   â”‚   â”‚   â””â”€â”€ AuthContext.tsx   # JWT auth state (React Context)
â”‚   â”‚   â”œâ”€â”€ lib/
â”‚   â”‚   â”‚   â””â”€â”€ api.ts            # Centralized API client
â”‚   â”‚   â””â”€â”€ types/
â”‚   â”‚       â””â”€â”€ index.ts          # TypeScript definitions
â”‚   â”œâ”€â”€ public/               # Static assets
â”‚   â””â”€â”€ vite.config.ts
â”‚
â”œâ”€â”€ server/                    # FastAPI backend
â”‚   â”œâ”€â”€ main.py               # App entry, CORS, router registration
â”‚   â”œâ”€â”€ config.py             # Environment variables (Pydantic)
â”‚   â”œâ”€â”€ database.py           # SQLAlchemy setup
â”‚   â”œâ”€â”€ models/               # ORM models
â”‚   â”‚   â”œâ”€â”€ user.py
â”‚   â”‚   â”œâ”€â”€ project.py
â”‚   â”‚   â”œâ”€â”€ document.py
â”‚   â”‚   â”œâ”€â”€ code_insight.py
â”‚   â”‚   â”œâ”€â”€ task.py
â”‚   â”‚   â”œâ”€â”€ embedding.py
â”‚   â”‚   â””â”€â”€ chat_message.py
â”‚   â”œâ”€â”€ schemas/              # Pydantic request/response models
â”‚   â”‚   â”œâ”€â”€ auth.py
â”‚   â”‚   â”œâ”€â”€ project.py
â”‚   â”‚   â”œâ”€â”€ learning.py
â”‚   â”‚   â””â”€â”€ developer.py
â”‚   â”œâ”€â”€ routers/              # API endpoints
â”‚   â”‚   â”œâ”€â”€ auth.py           # POST /register, /login, GET /me
â”‚   â”‚   â”œâ”€â”€ projects.py       # CRUD for projects
â”‚   â”‚   â”œâ”€â”€ learning.py       # Document processing
â”‚   â”‚   â”œâ”€â”€ developer.py      # Code analysis
â”‚   â”‚   â””â”€â”€ workflow.py       # Task extraction (Phase 7)
â”‚   â”œâ”€â”€ services/             # Business logic
â”‚   â”‚   â”œâ”€â”€ llm_service.py    # Multi-provider LLM abstraction
â”‚   â”‚   â”œâ”€â”€ pdf_service.py    # PDF extraction and chunking
â”‚   â”‚   â”œâ”€â”€ embedding_service.py  # Vector generation
â”‚   â”‚   â”œâ”€â”€ file_storage.py   # Appwrite Storage wrapper
â”‚   â”‚   â”œâ”€â”€ learning_service.py   # Document orchestration
â”‚   â”‚   â”œâ”€â”€ developer_service.py  # Code analysis orchestration
â”‚   â”‚   â””â”€â”€ prompts/          # System prompts
â”‚   â”‚       â”œâ”€â”€ learning_prompts.py
â”‚   â”‚       â””â”€â”€ developer_prompts.py
â”‚   â”œâ”€â”€ middleware/
â”‚   â”‚   â””â”€â”€ auth.py           # JWT validation dependency
â”‚   â”œâ”€â”€ migrations/           # SQL migration files
â”‚   â”‚   â”œâ”€â”€ 001_create_users.sql
â”‚   â”‚   â””â”€â”€ 002_create_project_tables.sql
â”‚   â”œâ”€â”€ tests/
â”‚   â”‚   â”œâ”€â”€ test_llm_service.py
â”‚   â”‚   â”œâ”€â”€ test_pdf_service.py
â”‚   â”‚   â””â”€â”€ test_embedding_service.py
â”‚   â””â”€â”€ requirements.txt
â”‚
â”œâ”€â”€ plan.md                   # 10-phase execution plan
â””â”€â”€ package.json              # Root workspace config
```

## ğŸš€ Getting Started

### Prerequisites

- **Node.js** 20+ and npm
- **Python** 3.11+ and pip
- **Neon Account** (free tier PostgreSQL with pgvector)
- **Appwrite Account** (free tier for file storage)
- **API Keys**:
  - Google AI Studio (Gemini API)
  - Groq API (optional, for fallback)
  - GitHub OAuth App (for social login)

### 1. Clone and Install

```bash
# Clone the repository
git clone <your-repo-url>
cd work-flow

# Install frontend dependencies
cd client
npm install

# Install backend dependencies
cd ../server
python -m venv venv
source venv/bin/activate  # On Windows: .\venv\Scripts\Activate
pip install -r requirements.txt
```

### 2. Environment Setup

Create `server/.env` (or use root `.env`):

```bash
# â”€â”€ Database (Neon PostgreSQL) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
DATABASE_URL=postgresql://user:password@ep-xxx.us-east-1.aws.neon.tech/neondb?sslmode=require

# â”€â”€ Appwrite (File Storage) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
APPWRITE_ENDPOINT=https://cloud.appwrite.io/v1
APPWRITE_PROJECT_ID=your-appwrite-project-id
APPWRITE_API_KEY=your-appwrite-api-key
APPWRITE_BUCKET_ID=your-bucket-id

# â”€â”€ Google Gemini API â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
GEMINI_API_KEY=your-gemini-api-key

# â”€â”€ Groq API (Fallback) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
GROQ_API_KEY=your-groq-api-key

# â”€â”€ Auth â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
JWT_SECRET=generate-a-random-secret
GITHUB_CLIENT_ID=your-github-oauth-client-id
GITHUB_CLIENT_SECRET=your-github-oauth-secret

# â”€â”€ Ollama (Local Inference â€” optional) â”€â”€â”€â”€â”€â”€
OLLAMA_BASE_URL=http://localhost:11434
OLLAMA_MODEL=phi3:mini
```

Create `client/.env`:

```bash
VITE_API_URL=http://localhost:8000
```

### 3. Database Setup

Run migrations against your Neon PostgreSQL database:

1. Go to your **Neon Dashboard** â†’ SQL Editor (or use `psql` with your `DATABASE_URL`)
2. **Enable pgvector**:
   ```sql
   CREATE EXTENSION IF NOT EXISTS vector;
   ```
3. Run `server/migrations/001_create_users.sql`
4. Run `server/migrations/002_create_project_tables.sql`

### 4. Start Development Servers

**Terminal 1 - Backend:**

```bash
cd server
python -m uvicorn main:app --reload --port 8000
```

**Terminal 2 - Frontend:**

```bash
cd client
npm run dev
```

Visit:

- **Frontend**: http://localhost:5173
- **Backend API Docs**: http://localhost:8000/docs
- **Health Check**: http://localhost:8000/api/health

## ğŸ“– Usage

### 1. Authentication

- Register with email/password or sign in with GitHub
- Access the dashboard at `/dashboard`

### 2. Create a Project

- Click "New Project" from the dashboard
- Set project name, goal, and constraints
- Add decisions and open questions as you work

### 3. Learning Module

- Navigate to your project â†’ "Learning" tab
- Upload PDF documents (study materials, documentation)
- Generate summaries (Short, Detailed, or Exam-ready)
- Extract key concepts with importance scores
- Get implementation steps for complex topics

### 4. Developer Module

- Go to "Developer" tab
- Paste code snippets
- Get structured explanations with component breakdowns
- Debug: identify bugs, edge cases, and improvements
- Generate README files from code

### 5. Workflow Module

- Go to "Workflow" tab
- Paste meeting transcripts or email threads
- Extract tasks with AI-assigned priorities
- Manage task status (pending/done)
- Update priorities as needs change

### 6. Coming Soon: Context-Aware Chat (Phase 8)

- Chat tab will provide AI assistance that references ALL your project content
- Ask: "How does my code implement concepts from the PDF I uploaded?"
- AI automatically retrieves relevant chunks from documents, code, and tasks
- See which sources influenced each response

## ğŸ§ª Testing

Run backend tests:

```bash
cd server
pytest
```

Test coverage includes:

- LLM service with provider fallback
- PDF extraction and chunking
- Embedding generation
- Vector similarity search

## ğŸ“Š Current Status

| Phase | Feature              | Status      |
| ----- | -------------------- | ----------- |
| 0     | Environment Setup    | âœ… Complete |
| 1     | Project Scaffold     | âœ… Complete |
| 2     | Authentication       | âœ… Complete |
| 3     | Project System       | âœ… Complete |
| 4     | LLM & PDF Services   | âœ… Complete |
| 5     | Learning Module      | âœ… Complete |
| 6     | Developer Module     | âœ… Complete |
| 7     | Workflow Module      | âœ… Complete |
| 8     | Context Engine & RAG | ğŸ”œ Next     |
| 9     | Drift Detection      | ğŸ“‹ Planned  |
| 10    | Deployment & Polish  | ğŸ“‹ Planned  |

**Current Phase**: Phases 1-7 fully implemented. Phase 8 (Context Engine) is the next major milestone.

## ğŸ¯ What Makes This Different?

### 1. **True Persistence**

Unlike ChatGPT or Claude, Workflow AI doesn't forget. Every document, code snippet, and task becomes part of your project's knowledge graph.

### 2. **Cross-Module Intelligence**

The Learning Module knows about your code. The Developer Module references your documents. The Workflow Module understands your project goals.

### 3. **Constraint-Aware AI**

Define "React only" or "Python 3.11+" â€” the AI respects and enforces your technical decisions.

### 4. **Hybrid Inference** (Coming in Phase 10)

Cloud performance when you need it. Local privacy when you want it. Your choice, same interface.

### 5. **Built for Real Workflows**

Not a chat toy. Designed for students managing coursework, developers building projects, and teams coordinating work.

## ğŸ› ï¸ Development

### Code Quality

- **Frontend**: TypeScript strict mode, ESLint, Prettier (coming)
- **Backend**: Type hints, Black formatting (coming), Pylint
- **Testing**: pytest for backend, Vitest for frontend (planned)

### Git Workflow

```bash
# Create feature branch
git checkout -b feature/context-engine

# Make changes, commit
git add .
git commit -m "feat: implement RAG pipeline"

# Push and create PR
git push origin feature/context-engine
```

### Environment Variables

Never commit secrets! Use `.env.example` as a template.

## ğŸš¢ Deployment (Phase 10)

### Frontend (Static Host)

```bash
# Build production bundle
cd client
npm run build
# Deploy the dist/ folder to Vercel, Netlify, or Cloudflare Pages
```

Set `VITE_API_URL` environment variable to your production backend URL.

### Backend (Render)

1. Create `Dockerfile` in `server/`
2. Add `render.yaml` at root
3. Connect GitHub repo to Render
4. Set environment variables
5. Deploy

### Post-Deployment Checklist

- [ ] Update `VITE_API_URL` to production backend URL
- [ ] Update CORS in FastAPI to allow production frontend domain
- [ ] Update GitHub OAuth redirect URI to production URL
- [ ] Test all features in production
- [ ] Set up monitoring (Sentry, LogRocket, etc.)

## ğŸ¤ Contributing

This project follows a **phase-based development model**. Each phase builds on the previous one, ensuring stability.

### Current Development Focus

**Phase 8: Context Persistence Engine & RAG**

Want to contribute? Check `plan.md` for detailed implementation specs.

## ğŸ“‹ Roadmap

### Q1 2026

- âœ… Core authentication and project management
- âœ… Learning, Developer, and Workflow modules
- ğŸ”œ Context Persistence Engine (Phase 8)
- ğŸ”œ RAG-powered conversations

### Q2 2026

- Drift detection and constraint enforcement
- Smart query routing
- Hybrid inference (cloud/local toggle)
- Production deployment

### Q3 2026

- Mobile app (React Native)
- Team collaboration features
- Advanced analytics dashboard
- Plugin system for custom modules

### Q4 2026

- Self-hosted option (Docker Compose)
- Enterprise features (SSO, audit logs)
- AMD Ryzen AI hardware acceleration
- Fine-tuned models for specific domains

## ğŸ“„ License

MIT License â€” see `LICENSE` file for details

## ğŸ™ Acknowledgments

- **React** & **Vite** for the blazing-fast frontend tooling
- **FastAPI** for the best Python web framework
- **Neon** for managed PostgreSQL with pgvector
- **Appwrite** for managed file storage
- **Google** for Gemini API
- **Groq** for fast LLM inference
- **Ollama** for local LLM runtime

## ğŸ“ Support

- **Issues**: Open an issue on GitHub
- **Discussions**: Use GitHub Discussions for questions
- **Email**: [your-email] (for security issues only)

---

**Built with â¤ï¸ for developers, students, and knowledge workers who deserve AI that actually remembers.**

_Project Status: Active Development | Current Phase: 7/10 Complete_
